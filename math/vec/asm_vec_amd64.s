// Code generated by command: go run asmgen.go -vec -out ../../math/vec/asm_vec_amd64.s -stubs ../../math/vec/asm_vec_stub_amd64.go -pkg=vec. DO NOT EDIT.

//go:build amd64 && !purego

#include "textflag.h"

DATA MASK_HI<>+0(SB)/8, $0xffffffff00000000
GLOBL MASK_HI<>(SB), RODATA|NOPTR, $8

// func addAssignUint32AVX2(v0 []uint32, v1 []uint32, vOut []uint32)
// Requires: AVX, AVX2
TEXT ·addAssignUint32AVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	MOVQ BX, SI
	SHRQ $0x03, SI
	SHLQ $0x03, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*4), Y0
	VMOVDQU (CX)(DI*4), Y1
	VPADDD  Y1, Y0, Y0
	VMOVDQU Y0, (DX)(DI*4)
	ADDQ    $0x08, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVL (AX)(DI*4), SI
	MOVL (CX)(DI*4), R8
	ADDL R8, SI
	MOVL SI, (DX)(DI*4)
	ADDQ $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func addAssignUint64AVX2(v0 []uint64, v1 []uint64, vOut []uint64)
// Requires: AVX, AVX2
TEXT ·addAssignUint64AVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	MOVQ BX, SI
	SHRQ $0x02, SI
	SHLQ $0x02, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*8), Y0
	VMOVDQU (CX)(DI*8), Y1
	VPADDQ  Y1, Y0, Y0
	VMOVDQU Y0, (DX)(DI*8)
	ADDQ    $0x04, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVQ (AX)(DI*8), SI
	MOVQ (CX)(DI*8), R8
	ADDQ R8, SI
	MOVQ SI, (DX)(DI*8)
	ADDQ $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func subAssignUint32AVX2(v0 []uint32, v1 []uint32, vOut []uint32)
// Requires: AVX, AVX2
TEXT ·subAssignUint32AVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	MOVQ BX, SI
	SHRQ $0x03, SI
	SHLQ $0x03, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*4), Y0
	VMOVDQU (CX)(DI*4), Y1
	VPSUBD  Y1, Y0, Y0
	VMOVDQU Y0, (DX)(DI*4)
	ADDQ    $0x08, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVL (AX)(DI*4), SI
	MOVL (CX)(DI*4), R8
	SUBL R8, SI
	MOVL SI, (DX)(DI*4)
	ADDQ $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func subAssignUint64AVX2(v0 []uint64, v1 []uint64, vOut []uint64)
// Requires: AVX, AVX2
TEXT ·subAssignUint64AVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	MOVQ BX, SI
	SHRQ $0x02, SI
	SHLQ $0x02, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*8), Y0
	VMOVDQU (CX)(DI*8), Y1
	VPSUBQ  Y1, Y0, Y0
	VMOVDQU Y0, (DX)(DI*8)
	ADDQ    $0x04, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVQ (AX)(DI*8), SI
	MOVQ (CX)(DI*8), R8
	SUBQ R8, SI
	MOVQ SI, (DX)(DI*8)
	ADDQ $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func scalarMulAssignUint32AVX2(v0 []uint32, c uint32, vOut []uint32)
// Requires: AVX, AVX2
TEXT ·scalarMulAssignUint32AVX2(SB), NOSPLIT, $0-56
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x03, BX
	SHLQ         $0x03, BX
	VPBROADCASTD c+24(FP), Y0
	XORQ         SI, SI
	JMP          loop_end

loop_body:
	VMOVDQU (AX)(SI*4), Y1
	VPMULLD Y0, Y1, Y1
	VMOVDQU Y1, (CX)(SI*4)
	ADDQ    $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	MOVL c+24(FP), BX
	JMP  leftover_loop_end

leftover_loop_body:
	MOVL  (AX)(SI*4), DI
	IMULL BX, DI
	MOVL  DI, (CX)(SI*4)
	ADDQ  $0x01, SI

leftover_loop_end:
	CMPQ SI, DX
	JL   leftover_loop_body
	RET

// func scalarMulAssignUint64AVX2(v0 []uint64, c uint64, vOut []uint64)
// Requires: AVX, AVX2
TEXT ·scalarMulAssignUint64AVX2(SB), NOSPLIT, $0-56
	VPBROADCASTQ MASK_HI<>+0(SB), Y0
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x02, BX
	SHLQ         $0x02, BX
	VPBROADCASTQ c+24(FP), Y1
	VPSHUFD      $0xb1, Y1, Y2
	XORQ         SI, SI
	JMP          loop_end

loop_body:
	VMOVDQU  (AX)(SI*8), Y3
	VPMULUDQ Y1, Y3, Y4
	VPMULLD  Y2, Y3, Y3
	VPSLLQ   $0x20, Y3, Y5
	VPAND    Y0, Y3, Y3
	VPADDQ   Y5, Y3, Y3
	VPADDQ   Y4, Y3, Y3
	VMOVDQU  Y3, (CX)(SI*8)
	ADDQ     $0x04, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	MOVQ c+24(FP), BX
	JMP  leftover_loop_end

leftover_loop_body:
	MOVQ  (AX)(SI*8), DI
	IMULQ BX, DI
	MOVQ  DI, (CX)(SI*8)
	ADDQ  $0x01, SI

leftover_loop_end:
	CMPQ SI, DX
	JL   leftover_loop_body
	RET

// func scalarMulAddAssignUint32AVX2(v0 []uint32, c uint32, vOut []uint32)
// Requires: AVX, AVX2
TEXT ·scalarMulAddAssignUint32AVX2(SB), NOSPLIT, $0-56
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x03, BX
	SHLQ         $0x03, BX
	VPBROADCASTD c+24(FP), Y0
	XORQ         SI, SI
	JMP          loop_end

loop_body:
	VMOVDQU (AX)(SI*4), Y1
	VMOVDQU (CX)(SI*4), Y2
	VPMULLD Y0, Y1, Y1
	VPADDD  Y1, Y2, Y2
	VMOVDQU Y2, (CX)(SI*4)
	ADDQ    $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	MOVL c+24(FP), BX
	JMP  leftover_loop_end

leftover_loop_body:
	MOVL  (AX)(SI*4), DI
	MOVL  (CX)(SI*4), R8
	IMULL BX, DI
	ADDL  DI, R8
	MOVL  R8, (CX)(SI*4)
	ADDQ  $0x01, SI

leftover_loop_end:
	CMPQ SI, DX
	JL   leftover_loop_body
	RET

// func scalarMulAddAssignUint64AVX2(v0 []uint64, c uint64, vOut []uint64)
// Requires: AVX, AVX2
TEXT ·scalarMulAddAssignUint64AVX2(SB), NOSPLIT, $0-56
	VPBROADCASTQ MASK_HI<>+0(SB), Y0
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x02, BX
	SHLQ         $0x02, BX
	VPBROADCASTQ c+24(FP), Y1
	VPSHUFD      $0xb1, Y1, Y2
	XORQ         SI, SI
	JMP          loop_end

loop_body:
	VMOVDQU  (AX)(SI*8), Y3
	VMOVDQU  (CX)(SI*8), Y4
	VPMULUDQ Y1, Y3, Y5
	VPMULLD  Y2, Y3, Y3
	VPSLLQ   $0x20, Y3, Y6
	VPAND    Y0, Y3, Y3
	VPADDQ   Y6, Y3, Y3
	VPADDQ   Y5, Y3, Y3
	VPADDQ   Y3, Y4, Y4
	VMOVDQU  Y4, (CX)(SI*8)
	ADDQ     $0x04, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	MOVQ c+24(FP), BX
	JMP  leftover_loop_end

leftover_loop_body:
	MOVQ  (AX)(SI*8), DI
	MOVQ  (CX)(SI*8), R8
	IMULQ BX, DI
	ADDQ  DI, R8
	MOVQ  R8, (CX)(SI*8)
	ADDQ  $0x01, SI

leftover_loop_end:
	CMPQ SI, DX
	JL   leftover_loop_body
	RET

// func scalarMulSubAssignUint32AVX2(v0 []uint32, c uint32, vOut []uint32)
// Requires: AVX, AVX2
TEXT ·scalarMulSubAssignUint32AVX2(SB), NOSPLIT, $0-56
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x03, BX
	SHLQ         $0x03, BX
	VPBROADCASTD c+24(FP), Y0
	XORQ         SI, SI
	JMP          loop_end

loop_body:
	VMOVDQU (AX)(SI*4), Y1
	VMOVDQU (CX)(SI*4), Y2
	VPMULLD Y0, Y1, Y1
	VPSUBD  Y1, Y2, Y2
	VMOVDQU Y2, (CX)(SI*4)
	ADDQ    $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	MOVL c+24(FP), BX
	JMP  leftover_loop_end

leftover_loop_body:
	MOVL  (AX)(SI*4), DI
	MOVL  (CX)(SI*4), R8
	IMULL BX, DI
	SUBL  DI, R8
	MOVL  R8, (CX)(SI*4)
	ADDQ  $0x01, SI

leftover_loop_end:
	CMPQ SI, DX
	JL   leftover_loop_body
	RET

// func scalarMulSubAssignUint64AVX2(v0 []uint64, c uint64, vOut []uint64)
// Requires: AVX, AVX2
TEXT ·scalarMulSubAssignUint64AVX2(SB), NOSPLIT, $0-56
	VPBROADCASTQ MASK_HI<>+0(SB), Y0
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x02, BX
	SHLQ         $0x02, BX
	VPBROADCASTQ c+24(FP), Y1
	VPSHUFD      $0xb1, Y1, Y2
	XORQ         SI, SI
	JMP          loop_end

loop_body:
	VMOVDQU  (AX)(SI*8), Y3
	VMOVDQU  (CX)(SI*8), Y4
	VPMULUDQ Y1, Y3, Y5
	VPMULLD  Y2, Y3, Y3
	VPSLLQ   $0x20, Y3, Y6
	VPAND    Y0, Y3, Y3
	VPADDQ   Y6, Y3, Y3
	VPADDQ   Y5, Y3, Y3
	VPSUBQ   Y3, Y4, Y4
	VMOVDQU  Y4, (CX)(SI*8)
	ADDQ     $0x04, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	MOVQ c+24(FP), BX
	JMP  leftover_loop_end

leftover_loop_body:
	MOVQ  (AX)(SI*8), DI
	MOVQ  (CX)(SI*8), R8
	IMULQ BX, DI
	SUBQ  DI, R8
	MOVQ  R8, (CX)(SI*8)
	ADDQ  $0x01, SI

leftover_loop_end:
	CMPQ SI, DX
	JL   leftover_loop_body
	RET

// func elementWiseMulAssignUint32AVX2(v0 []uint32, v1 []uint32, vOut []uint32)
// Requires: AVX, AVX2
TEXT ·elementWiseMulAssignUint32AVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	MOVQ BX, SI
	SHRQ $0x03, SI
	SHLQ $0x03, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*4), Y0
	VMOVDQU (CX)(DI*4), Y1
	VPMULLD Y1, Y0, Y0
	VMOVDQU Y0, (DX)(DI*4)
	ADDQ    $0x08, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVL  (AX)(DI*4), SI
	MOVL  (CX)(DI*4), R8
	IMULL R8, SI
	MOVL  SI, (DX)(DI*4)
	ADDQ  $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func elementWiseMulAssignUint64AVX2(v0 []uint64, v1 []uint64, vOut []uint64)
// Requires: AVX, AVX2
TEXT ·elementWiseMulAssignUint64AVX2(SB), NOSPLIT, $0-72
	VPBROADCASTQ MASK_HI<>+0(SB), Y0
	MOVQ         v0_base+0(FP), AX
	MOVQ         v1_base+24(FP), CX
	MOVQ         vOut_base+48(FP), DX
	MOVQ         vOut_len+56(FP), BX
	MOVQ         BX, SI
	SHRQ         $0x02, SI
	SHLQ         $0x02, SI
	XORQ         DI, DI
	JMP          loop_end

loop_body:
	VMOVDQU  (AX)(DI*8), Y1
	VMOVDQU  (CX)(DI*8), Y2
	VPSHUFD  $0xb1, Y1, Y3
	VPMULUDQ Y1, Y2, Y1
	VPMULLD  Y3, Y2, Y2
	VPSLLQ   $0x20, Y2, Y3
	VPAND    Y0, Y2, Y2
	VPADDQ   Y3, Y2, Y2
	VPADDQ   Y1, Y2, Y2
	VMOVDQU  Y2, (DX)(DI*8)
	ADDQ     $0x04, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVQ  (AX)(DI*8), SI
	MOVQ  (CX)(DI*8), R8
	IMULQ R8, SI
	MOVQ  SI, (DX)(DI*8)
	ADDQ  $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func elementWiseMulAddAssignUint32AVX2(v0 []uint32, v1 []uint32, vOut []uint32)
// Requires: AVX, AVX2
TEXT ·elementWiseMulAddAssignUint32AVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	MOVQ BX, SI
	SHRQ $0x03, SI
	SHLQ $0x03, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*4), Y0
	VMOVDQU (CX)(DI*4), Y1
	VMOVDQU (DX)(DI*4), Y2
	VPMULLD Y1, Y0, Y0
	VPADDD  Y0, Y2, Y2
	VMOVDQU Y2, (DX)(DI*4)
	ADDQ    $0x08, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVL  (AX)(DI*4), SI
	MOVL  (CX)(DI*4), R8
	MOVL  (DX)(DI*4), R9
	IMULL R8, SI
	ADDL  SI, R9
	MOVL  R9, (DX)(DI*4)
	ADDQ  $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func elementWiseMulAddAssignUint64AVX2(v0 []uint64, v1 []uint64, vOut []uint64)
// Requires: AVX, AVX2
TEXT ·elementWiseMulAddAssignUint64AVX2(SB), NOSPLIT, $0-72
	VPBROADCASTQ MASK_HI<>+0(SB), Y0
	MOVQ         v0_base+0(FP), AX
	MOVQ         v1_base+24(FP), CX
	MOVQ         vOut_base+48(FP), DX
	MOVQ         vOut_len+56(FP), BX
	MOVQ         BX, SI
	SHRQ         $0x02, SI
	SHLQ         $0x02, SI
	XORQ         DI, DI
	JMP          loop_end

loop_body:
	VMOVDQU  (AX)(DI*8), Y1
	VMOVDQU  (CX)(DI*8), Y2
	VMOVDQU  (DX)(DI*8), Y3
	VPSHUFD  $0xb1, Y1, Y4
	VPMULUDQ Y1, Y2, Y1
	VPMULLD  Y4, Y2, Y2
	VPSLLQ   $0x20, Y2, Y4
	VPAND    Y0, Y2, Y2
	VPADDQ   Y4, Y2, Y2
	VPADDQ   Y1, Y2, Y2
	VPADDQ   Y2, Y3, Y3
	VMOVDQU  Y3, (DX)(DI*8)
	ADDQ     $0x04, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVQ  (AX)(DI*8), SI
	MOVQ  (CX)(DI*8), R8
	MOVQ  (DX)(DI*8), R9
	IMULQ R8, SI
	ADDQ  SI, R9
	MOVQ  R9, (DX)(DI*8)
	ADDQ  $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func elementWiseMulSubAssignUint32AVX2(v0 []uint32, v1 []uint32, vOut []uint32)
// Requires: AVX, AVX2
TEXT ·elementWiseMulSubAssignUint32AVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	MOVQ BX, SI
	SHRQ $0x03, SI
	SHLQ $0x03, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*4), Y0
	VMOVDQU (CX)(DI*4), Y1
	VMOVDQU (DX)(DI*4), Y2
	VPMULLD Y1, Y0, Y0
	VPSUBD  Y0, Y2, Y2
	VMOVDQU Y2, (DX)(DI*4)
	ADDQ    $0x08, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVL  (AX)(DI*4), SI
	MOVL  (CX)(DI*4), R8
	MOVL  (DX)(DI*4), R9
	IMULL R8, SI
	SUBL  SI, R9
	MOVL  R9, (DX)(DI*4)
	ADDQ  $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET

// func elementWiseMulSubAssignUint64AVX2(v0 []uint64, v1 []uint64, vOut []uint64)
// Requires: AVX, AVX2
TEXT ·elementWiseMulSubAssignUint64AVX2(SB), NOSPLIT, $0-72
	VPBROADCASTQ MASK_HI<>+0(SB), Y0
	MOVQ         v0_base+0(FP), AX
	MOVQ         v1_base+24(FP), CX
	MOVQ         vOut_base+48(FP), DX
	MOVQ         vOut_len+56(FP), BX
	MOVQ         BX, SI
	SHRQ         $0x02, SI
	SHLQ         $0x02, SI
	XORQ         DI, DI
	JMP          loop_end

loop_body:
	VMOVDQU  (AX)(DI*8), Y1
	VMOVDQU  (CX)(DI*8), Y2
	VMOVDQU  (DX)(DI*8), Y3
	VPSHUFD  $0xb1, Y1, Y4
	VPMULUDQ Y1, Y2, Y1
	VPMULLD  Y4, Y2, Y2
	VPSLLQ   $0x20, Y2, Y4
	VPAND    Y0, Y2, Y2
	VPADDQ   Y4, Y2, Y2
	VPADDQ   Y1, Y2, Y2
	VPSUBQ   Y2, Y3, Y3
	VMOVDQU  Y3, (DX)(DI*8)
	ADDQ     $0x04, DI

loop_end:
	CMPQ DI, SI
	JL   loop_body
	JMP  leftover_loop_end

leftover_loop_body:
	MOVQ  (AX)(DI*8), SI
	MOVQ  (CX)(DI*8), R8
	MOVQ  (DX)(DI*8), R9
	IMULQ R8, SI
	SUBQ  SI, R9
	MOVQ  R9, (DX)(DI*8)
	ADDQ  $0x01, DI

leftover_loop_end:
	CMPQ DI, BX
	JL   leftover_loop_body
	RET
