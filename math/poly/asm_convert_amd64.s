// Code generated by command: go run asmgen.go -convert -out ../../math/poly/asm_convert_amd64.s -stubs ../../math/poly/asm_convert_stub_amd64.go -pkg=poly. DO NOT EDIT.

//go:build amd64 && !purego

#include "textflag.h"

DATA EXP2_52<>+0(SB)/8, $(4503599627370496.0)
GLOBL EXP2_52<>(SB), RODATA|NOPTR, $8

DATA EXP2_84_63<>+0(SB)/8, $(19342822337206104000000000.0)
GLOBL EXP2_84_63<>(SB), RODATA|NOPTR, $8

DATA EXP2_84_63_52<>+0(SB)/8, $(19342822341709703000000000.0)
GLOBL EXP2_84_63_52<>(SB), RODATA|NOPTR, $8

DATA MANT_MASK<>+0(SB)/8, $0x000fffffffffffff
GLOBL MANT_MASK<>(SB), RODATA|NOPTR, $8

DATA BIT_MANT_MASK<>+0(SB)/8, $0x0010000000000000
GLOBL BIT_MANT_MASK<>(SB), RODATA|NOPTR, $8

DATA EXP_MASK<>+0(SB)/8, $0x00000000000007ff
GLOBL EXP_MASK<>(SB), RODATA|NOPTR, $8

DATA EXP_SHIFT<>+0(SB)/8, $0x000000000000043e
GLOBL EXP_SHIFT<>(SB), RODATA|NOPTR, $8

// func convertPolyToFourierPolyAssignUint32AVX2(p []uint32, fpOut []float64)
// Requires: AVX
TEXT ·convertPolyToFourierPolyAssignUint32AVX2(SB), NOSPLIT, $0-48
	MOVQ p_base+0(FP), AX
	MOVQ fpOut_base+24(FP), CX
	MOVQ fpOut_len+32(FP), DX
	MOVQ DX, BX
	SHRQ $0x01, BX
	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU   (AX)(DI*4), X0
	VMOVDQU   (AX)(BX*4), X1
	VCVTDQ2PD X0, Y0
	VCVTDQ2PD X1, Y1
	VMOVUPD   Y0, (CX)(SI*8)
	VMOVUPD   Y1, 32(CX)(SI*8)
	ADDQ      $0x08, SI
	ADDQ      $0x04, DI
	ADDQ      $0x04, BX

loop_end:
	CMPQ SI, DX
	JL   loop_body
	RET

// func convertPolyToFourierPolyAssignUint64AVX2(p []uint64, fpOut []float64)
// Requires: AVX, AVX2
TEXT ·convertPolyToFourierPolyAssignUint64AVX2(SB), NOSPLIT, $0-48
	MOVQ         p_base+0(FP), AX
	MOVQ         fpOut_base+24(FP), CX
	MOVQ         fpOut_len+32(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x01, BX
	VBROADCASTSD EXP2_52<>+0(SB), Y0
	VBROADCASTSD EXP2_84_63<>+0(SB), Y1
	VBROADCASTSD EXP2_84_63_52<>+0(SB), Y2
	XORQ         SI, SI
	XORQ         DI, DI
	JMP          loop_end

loop_body:
	VMOVDQU  (AX)(DI*8), Y3
	VMOVDQU  (AX)(BX*8), Y4
	VPBLENDD $0x55, Y3, Y0, Y5
	VPSRLQ   $0x20, Y3, Y3
	VPXOR    Y1, Y3, Y3
	VSUBPD   Y2, Y3, Y3
	VADDPD   Y3, Y5, Y3
	VPBLENDD $0x55, Y4, Y0, Y5
	VPSRLQ   $0x20, Y4, Y4
	VPXOR    Y1, Y4, Y4
	VSUBPD   Y2, Y4, Y4
	VADDPD   Y4, Y5, Y4
	VMOVUPD  Y3, (CX)(SI*8)
	VMOVUPD  Y4, 32(CX)(SI*8)
	ADDQ     $0x08, SI
	ADDQ     $0x04, DI
	ADDQ     $0x04, BX

loop_end:
	CMPQ SI, DX
	JL   loop_body
	RET

// func floatModQInPlaceAVX2(coeffs []float64, Q float64, QInv float64)
// Requires: AVX
TEXT ·floatModQInPlaceAVX2(SB), NOSPLIT, $0-40
	MOVQ         coeffs_base+0(FP), AX
	MOVQ         coeffs_len+8(FP), CX
	VBROADCASTSD Q+24(FP), Y0
	VBROADCASTSD QInv+32(FP), Y1
	XORQ         DX, DX
	JMP          loop_end

loop_body:
	VMOVUPD  (AX)(DX*8), Y2
	VMULPD   Y1, Y2, Y2
	VROUNDPD $0x00, Y2, Y3
	VSUBPD   Y3, Y2, Y2
	VMULPD   Y0, Y2, Y2
	VROUNDPD $0x00, Y2, Y2
	VMOVUPD  Y2, (AX)(DX*8)
	ADDQ     $0x04, DX

loop_end:
	CMPQ DX, CX
	JL   loop_body
	RET

// func convertFourierPolyToPolyAssignUint32AVX2(fp []float64, pOut []uint32)
// Requires: AVX
TEXT ·convertFourierPolyToPolyAssignUint32AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), CX
	MOVQ fp_len+8(FP), DX
	MOVQ DX, BX
	SHRQ $0x01, BX
	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD    (AX)(SI*8), Y0
	VMOVUPD    32(AX)(SI*8), Y1
	VCVTPD2DQY Y0, X0
	VCVTPD2DQY Y1, X1
	VMOVDQU    X0, (CX)(DI*4)
	VMOVDQU    X1, (CX)(BX*4)
	ADDQ       $0x08, SI
	ADDQ       $0x04, DI
	ADDQ       $0x04, BX

loop_end:
	CMPQ SI, DX
	JL   loop_body
	RET

// func convertFourierPolyToPolyAssignUint64AVX2(fp []float64, pOut []uint64)
// Requires: AVX, AVX2
TEXT ·convertFourierPolyToPolyAssignUint64AVX2(SB), NOSPLIT, $0-48
	MOVQ         fp_base+0(FP), AX
	MOVQ         pOut_base+24(FP), CX
	MOVQ         fp_len+8(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x01, BX
	VBROADCASTSD MANT_MASK<>+0(SB), Y0
	VBROADCASTSD BIT_MANT_MASK<>+0(SB), Y1
	VBROADCASTSD EXP_MASK<>+0(SB), Y2
	VBROADCASTSD EXP_SHIFT<>+0(SB), Y3
	VPXOR        Y4, Y4, Y4
	XORQ         SI, SI
	XORQ         DI, DI
	JMP          loop_end

loop_body:
	VMOVUPD   (AX)(SI*8), Y5
	VMOVUPD   32(AX)(SI*8), Y6
	VANDPD    Y0, Y5, Y7
	VORPD     Y1, Y7, Y7
	VPSRLQ    $0x34, Y5, Y8
	VANDPD    Y2, Y8, Y8
	VPSRLQ    $0x3f, Y5, Y5
	VPSUBQ    Y5, Y4, Y5
	VPSLLQ    $0x0b, Y7, Y7
	VPSUBQ    Y8, Y3, Y8
	VPSRLVQ   Y8, Y7, Y7
	VPSUBQ    Y7, Y4, Y8
	VPBLENDVB Y5, Y8, Y7, Y5
	VANDPD    Y0, Y6, Y7
	VORPD     Y1, Y7, Y7
	VPSRLQ    $0x34, Y6, Y8
	VANDPD    Y2, Y8, Y8
	VPSRLQ    $0x3f, Y6, Y6
	VPSUBQ    Y6, Y4, Y6
	VPSLLQ    $0x0b, Y7, Y7
	VPSUBQ    Y8, Y3, Y8
	VPSRLVQ   Y8, Y7, Y7
	VPSUBQ    Y7, Y4, Y8
	VPBLENDVB Y6, Y8, Y7, Y6
	VMOVDQU   Y5, (CX)(DI*8)
	VMOVDQU   Y6, (CX)(BX*8)
	ADDQ      $0x08, SI
	ADDQ      $0x04, DI
	ADDQ      $0x04, BX

loop_end:
	CMPQ SI, DX
	JL   loop_body
	RET

// func convertFourierPolyToPolyAddAssignUint32AVX2(fp []float64, pOut []uint32)
// Requires: AVX
TEXT ·convertFourierPolyToPolyAddAssignUint32AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), CX
	MOVQ fp_len+8(FP), DX
	MOVQ DX, BX
	SHRQ $0x01, BX
	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD    (AX)(SI*8), Y0
	VMOVUPD    32(AX)(SI*8), Y1
	VMOVUPD    (CX)(DI*4), X2
	VMOVUPD    (CX)(BX*4), X3
	VCVTPD2DQY Y0, X0
	VCVTPD2DQY Y1, X1
	VPADDD     X0, X2, X2
	VPADDD     X1, X3, X3
	VMOVDQU    X2, (CX)(DI*4)
	VMOVDQU    X3, (CX)(BX*4)
	ADDQ       $0x08, SI
	ADDQ       $0x04, DI
	ADDQ       $0x04, BX

loop_end:
	CMPQ SI, DX
	JL   loop_body
	RET

// func convertFourierPolyToPolyAddAssignUint64AVX2(fp []float64, pOut []uint64)
// Requires: AVX, AVX2
TEXT ·convertFourierPolyToPolyAddAssignUint64AVX2(SB), NOSPLIT, $0-48
	MOVQ         fp_base+0(FP), AX
	MOVQ         pOut_base+24(FP), CX
	MOVQ         fp_len+8(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x01, BX
	VBROADCASTSD MANT_MASK<>+0(SB), Y0
	VBROADCASTSD BIT_MANT_MASK<>+0(SB), Y1
	VBROADCASTSD EXP_MASK<>+0(SB), Y2
	VBROADCASTSD EXP_SHIFT<>+0(SB), Y3
	VPXOR        Y4, Y4, Y4
	XORQ         SI, SI
	XORQ         DI, DI
	JMP          loop_end

loop_body:
	VMOVUPD   (AX)(SI*8), Y5
	VMOVUPD   32(AX)(SI*8), Y6
	VMOVDQU   (CX)(DI*8), Y7
	VMOVDQU   (CX)(BX*8), Y8
	VANDPD    Y0, Y5, Y9
	VORPD     Y1, Y9, Y9
	VPSRLQ    $0x34, Y5, Y10
	VANDPD    Y2, Y10, Y10
	VPSRLQ    $0x3f, Y5, Y5
	VPSUBQ    Y5, Y4, Y5
	VPSLLQ    $0x0b, Y9, Y9
	VPSUBQ    Y10, Y3, Y10
	VPSRLVQ   Y10, Y9, Y9
	VPSUBQ    Y9, Y4, Y10
	VPBLENDVB Y5, Y10, Y9, Y5
	VANDPD    Y0, Y6, Y9
	VORPD     Y1, Y9, Y9
	VPSRLQ    $0x34, Y6, Y10
	VANDPD    Y2, Y10, Y10
	VPSRLQ    $0x3f, Y6, Y6
	VPSUBQ    Y6, Y4, Y6
	VPSLLQ    $0x0b, Y9, Y9
	VPSUBQ    Y10, Y3, Y10
	VPSRLVQ   Y10, Y9, Y9
	VPSUBQ    Y9, Y4, Y10
	VPBLENDVB Y6, Y10, Y9, Y6
	VPADDQ    Y5, Y7, Y7
	VPADDQ    Y6, Y8, Y8
	VMOVDQU   Y7, (CX)(DI*8)
	VMOVDQU   Y8, (CX)(BX*8)
	ADDQ      $0x08, SI
	ADDQ      $0x04, DI
	ADDQ      $0x04, BX

loop_end:
	CMPQ SI, DX
	JL   loop_body
	RET

// func convertFourierPolyToPolySubAssignUint32AVX2(fp []float64, pOut []uint32)
// Requires: AVX
TEXT ·convertFourierPolyToPolySubAssignUint32AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), CX
	MOVQ fp_len+8(FP), DX
	MOVQ DX, BX
	SHRQ $0x01, BX
	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD    (AX)(SI*8), Y0
	VMOVUPD    32(AX)(SI*8), Y1
	VMOVUPD    (CX)(DI*4), X2
	VMOVUPD    (CX)(BX*4), X3
	VCVTPD2DQY Y0, X0
	VCVTPD2DQY Y1, X1
	VPSUBD     X0, X2, X2
	VPSUBD     X1, X3, X3
	VMOVDQU    X2, (CX)(DI*4)
	VMOVDQU    X3, (CX)(BX*4)
	ADDQ       $0x08, SI
	ADDQ       $0x04, DI
	ADDQ       $0x04, BX

loop_end:
	CMPQ SI, DX
	JL   loop_body
	RET

// func convertFourierPolyToPolySubAssignUint64AVX2(fp []float64, pOut []uint64)
// Requires: AVX, AVX2
TEXT ·convertFourierPolyToPolySubAssignUint64AVX2(SB), NOSPLIT, $0-48
	MOVQ         fp_base+0(FP), AX
	MOVQ         pOut_base+24(FP), CX
	MOVQ         fp_len+8(FP), DX
	MOVQ         DX, BX
	SHRQ         $0x01, BX
	VBROADCASTSD MANT_MASK<>+0(SB), Y0
	VBROADCASTSD BIT_MANT_MASK<>+0(SB), Y1
	VBROADCASTSD EXP_MASK<>+0(SB), Y2
	VBROADCASTSD EXP_SHIFT<>+0(SB), Y3
	VPXOR        Y4, Y4, Y4
	XORQ         SI, SI
	XORQ         DI, DI
	JMP          loop_end

loop_body:
	VMOVUPD   (AX)(SI*8), Y5
	VMOVUPD   32(AX)(SI*8), Y6
	VMOVDQU   (CX)(DI*8), Y7
	VMOVDQU   (CX)(BX*8), Y8
	VANDPD    Y0, Y5, Y9
	VORPD     Y1, Y9, Y9
	VPSRLQ    $0x34, Y5, Y10
	VANDPD    Y2, Y10, Y10
	VPSRLQ    $0x3f, Y5, Y5
	VPSUBQ    Y5, Y4, Y5
	VPSLLQ    $0x0b, Y9, Y9
	VPSUBQ    Y10, Y3, Y10
	VPSRLVQ   Y10, Y9, Y9
	VPSUBQ    Y9, Y4, Y10
	VPBLENDVB Y5, Y10, Y9, Y5
	VANDPD    Y0, Y6, Y9
	VORPD     Y1, Y9, Y9
	VPSRLQ    $0x34, Y6, Y10
	VANDPD    Y2, Y10, Y10
	VPSRLQ    $0x3f, Y6, Y6
	VPSUBQ    Y6, Y4, Y6
	VPSLLQ    $0x0b, Y9, Y9
	VPSUBQ    Y10, Y3, Y10
	VPSRLVQ   Y10, Y9, Y9
	VPSUBQ    Y9, Y4, Y10
	VPBLENDVB Y6, Y10, Y9, Y6
	VPSUBQ    Y5, Y7, Y7
	VPSUBQ    Y6, Y8, Y8
	VMOVDQU   Y7, (CX)(DI*8)
	VMOVDQU   Y8, (CX)(BX*8)
	ADDQ      $0x08, SI
	ADDQ      $0x04, DI
	ADDQ      $0x04, BX

loop_end:
	CMPQ SI, DX
	JL   loop_body
	RET
