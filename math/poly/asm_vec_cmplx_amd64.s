// Code generated by command: go run asmgen.go -vec_cmplx -out ../../math/poly/asm_vec_cmplx_amd64.s -stubs ../../math/poly/asm_vec_cmplx_stub_amd64.go -pkg=poly. DO NOT EDIT.

//go:build amd64 && !purego

#include "textflag.h"

// func addCmplxAssignAVX2(v0 []float64, v1 []float64, vOut []float64)
// Requires: AVX
TEXT ·addCmplxAssignAVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0
	VMOVUPD (CX)(SI*8), Y1
	VADDPD  Y1, Y0, Y0
	VMOVUPD Y0, (DX)(SI*8)
	ADDQ    $0x04, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET

// func subCmplxAssignAVX2(v0 []float64, v1 []float64, vOut []float64)
// Requires: AVX
TEXT ·subCmplxAssignAVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0
	VMOVUPD (CX)(SI*8), Y1
	VSUBPD  Y1, Y0, Y0
	VMOVUPD Y0, (DX)(SI*8)
	ADDQ    $0x04, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET

// func negCmplxAssignAVX2(v0 []float64, vOut []float64)
// Requires: AVX
TEXT ·negCmplxAssignAVX2(SB), NOSPLIT, $0-48
	MOVQ   v0_base+0(FP), AX
	MOVQ   vOut_base+24(FP), CX
	MOVQ   vOut_len+32(FP), DX
	VXORPD Y0, Y0, Y0
	XORQ   BX, BX
	JMP    loop_end

loop_body:
	VMOVUPD (AX)(BX*8), Y1
	VSUBPD  Y1, Y0, Y1
	VMOVUPD Y1, (CX)(BX*8)
	ADDQ    $0x04, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func floatMulCmplxAssignAVX2(v0 []float64, c float64, vOut []float64)
// Requires: AVX
TEXT ·floatMulCmplxAssignAVX2(SB), NOSPLIT, $0-56
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	VBROADCASTSD c+24(FP), Y0
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD (AX)(BX*8), Y1
	VMULPD  Y0, Y1, Y1
	VMOVUPD Y1, (CX)(BX*8)
	ADDQ    $0x04, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func floatMulAddCmplxAssignAVX2(v0 []float64, c float64, vOut []float64)
// Requires: AVX, FMA3
TEXT ·floatMulAddCmplxAssignAVX2(SB), NOSPLIT, $0-56
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	VBROADCASTSD c+24(FP), Y0
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD     (AX)(BX*8), Y1
	VMOVUPD     (CX)(BX*8), Y2
	VFMADD231PD Y0, Y1, Y2
	VMOVUPD     Y2, (CX)(BX*8)
	ADDQ        $0x04, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func floatMulSubCmplxAssignAVX2(v0 []float64, c float64, vOut []float64)
// Requires: AVX, FMA3
TEXT ·floatMulSubCmplxAssignAVX2(SB), NOSPLIT, $0-56
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+32(FP), CX
	MOVQ         vOut_len+40(FP), DX
	VBROADCASTSD c+24(FP), Y0
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD      (AX)(BX*8), Y1
	VMOVUPD      (CX)(BX*8), Y2
	VFNMADD231PD Y0, Y1, Y2
	VMOVUPD      Y2, (CX)(BX*8)
	ADDQ         $0x04, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func cmplxMulCmplxAssignAVX2(v0 []float64, c complex128, vOut []float64)
// Requires: AVX, FMA3
TEXT ·cmplxMulCmplxAssignAVX2(SB), NOSPLIT, $0-64
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+40(FP), CX
	MOVQ         vOut_len+48(FP), DX
	VBROADCASTSD c_real+24(FP), Y0
	VBROADCASTSD c_imag+32(FP), Y1
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD      (AX)(BX*8), Y2
	VMOVUPD      32(AX)(BX*8), Y3
	VMULPD       Y0, Y2, Y4
	VFNMADD231PD Y1, Y3, Y4
	VMULPD       Y0, Y3, Y3
	VFMADD231PD  Y1, Y2, Y3
	VMOVUPD      Y4, (CX)(BX*8)
	VMOVUPD      Y3, 32(CX)(BX*8)
	ADDQ         $0x08, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func cmplxMulAddCmplxAssignAVX2(v0 []float64, c complex128, vOut []float64)
// Requires: AVX, FMA3
TEXT ·cmplxMulAddCmplxAssignAVX2(SB), NOSPLIT, $0-64
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+40(FP), CX
	MOVQ         vOut_len+48(FP), DX
	VBROADCASTSD c_real+24(FP), Y0
	VBROADCASTSD c_imag+32(FP), Y1
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD      (AX)(BX*8), Y2
	VMOVUPD      32(AX)(BX*8), Y3
	VMOVUPD      (CX)(BX*8), Y4
	VMOVUPD      32(CX)(BX*8), Y5
	VFMADD231PD  Y0, Y2, Y4
	VFNMADD231PD Y1, Y3, Y4
	VFMADD231PD  Y0, Y3, Y5
	VFMADD231PD  Y1, Y2, Y5
	VMOVUPD      Y4, (CX)(BX*8)
	VMOVUPD      Y5, 32(CX)(BX*8)
	ADDQ         $0x08, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func cmplxMulSubCmplxAssignAVX2(v0 []float64, c complex128, vOut []float64)
// Requires: AVX, FMA3
TEXT ·cmplxMulSubCmplxAssignAVX2(SB), NOSPLIT, $0-64
	MOVQ         v0_base+0(FP), AX
	MOVQ         vOut_base+40(FP), CX
	MOVQ         vOut_len+48(FP), DX
	VBROADCASTSD c_real+24(FP), Y0
	VBROADCASTSD c_imag+32(FP), Y1
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD      (AX)(BX*8), Y2
	VMOVUPD      32(AX)(BX*8), Y3
	VMOVUPD      (CX)(BX*8), Y4
	VMOVUPD      32(CX)(BX*8), Y5
	VFNMADD231PD Y0, Y2, Y4
	VFMADD231PD  Y1, Y3, Y4
	VFNMADD231PD Y0, Y3, Y5
	VFNMADD231PD Y1, Y2, Y5
	VMOVUPD      Y4, (CX)(BX*8)
	VMOVUPD      Y5, 32(CX)(BX*8)
	ADDQ         $0x08, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func elementWiseMulCmplxAssignAVX2(v0 []float64, v1 []float64, vOut []float64)
// Requires: AVX, FMA3
TEXT ·elementWiseMulCmplxAssignAVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD      (AX)(SI*8), Y0
	VMOVUPD      32(AX)(SI*8), Y1
	VMOVUPD      (CX)(SI*8), Y2
	VMOVUPD      32(CX)(SI*8), Y3
	VMULPD       Y0, Y2, Y4
	VFNMADD231PD Y1, Y3, Y4
	VMULPD       Y0, Y3, Y0
	VFMADD231PD  Y1, Y2, Y0
	VMOVUPD      Y4, (DX)(SI*8)
	VMOVUPD      Y0, 32(DX)(SI*8)
	ADDQ         $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET

// func elementWiseMulAddCmplxAssignAVX2(v0 []float64, v1 []float64, vOut []float64)
// Requires: AVX, FMA3
TEXT ·elementWiseMulAddCmplxAssignAVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD      (AX)(SI*8), Y0
	VMOVUPD      32(AX)(SI*8), Y1
	VMOVUPD      (CX)(SI*8), Y2
	VMOVUPD      32(CX)(SI*8), Y3
	VMOVUPD      (DX)(SI*8), Y4
	VMOVUPD      32(DX)(SI*8), Y5
	VFMADD231PD  Y0, Y2, Y4
	VFNMADD231PD Y1, Y3, Y4
	VFMADD231PD  Y0, Y3, Y5
	VFMADD231PD  Y1, Y2, Y5
	VMOVUPD      Y4, (DX)(SI*8)
	VMOVUPD      Y5, 32(DX)(SI*8)
	ADDQ         $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET

// func elementWiseMulSubCmplxAssignAVX2(v0 []float64, v1 []float64, vOut []float64)
// Requires: AVX, FMA3
TEXT ·elementWiseMulSubCmplxAssignAVX2(SB), NOSPLIT, $0-72
	MOVQ v0_base+0(FP), AX
	MOVQ v1_base+24(FP), CX
	MOVQ vOut_base+48(FP), DX
	MOVQ vOut_len+56(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD      (AX)(SI*8), Y0
	VMOVUPD      32(AX)(SI*8), Y1
	VMOVUPD      (CX)(SI*8), Y2
	VMOVUPD      32(CX)(SI*8), Y3
	VMOVUPD      (DX)(SI*8), Y4
	VMOVUPD      32(DX)(SI*8), Y5
	VFNMADD231PD Y0, Y2, Y4
	VFMADD231PD  Y1, Y3, Y4
	VFNMADD231PD Y0, Y3, Y5
	VFNMADD231PD Y1, Y2, Y5
	VMOVUPD      Y4, (DX)(SI*8)
	VMOVUPD      Y5, 32(DX)(SI*8)
	ADDQ         $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET
